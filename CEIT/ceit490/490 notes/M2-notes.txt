What is Deep Learning?
Deep learning: Subset of machine learning, inspired by the brainâ€™s connections.
Applications: Language translation, self-driving cars, chatbots, medical diagnostics.
Difference: Traditional ML requires feature engineering; deep learning enables feature learning from raw data.
Requirement: Large datasets.
PyTorch Overview
PyTorch: Popular deep learning framework, user-friendly, similar to NumPy.
Libraries: torchvision (images), torchaudio (audio), torchtext (text).
Tensors in PyTorch
Definition: Tensors are multidimensional arrays, essential in PyTorch.
Attributes: tensor.shape, tensor.device, tensor.dtype (important for GPU acceleration).
Operations: Element-wise addition/multiplication, transposition, matrix multiplication.
Neural Networks
Linear layer: Takes input, applies a linear function, returns output.
Creating a network: Use nn.Sequential() to stack multiple linear layers.
Activation functions: Add non-linearity, enabling learning of complex relationships (e.g., sigmoid, softmax).
Activation Functions
Sigmoid: Used for binary classification, outputs probability between 0 and 1.
Softmax: Used for multi-class classification, outputs a probability distribution across classes.
--------------------------------------------------------

Running a Forward Pass
Definition: A forward pass refers to passing input data through a neural network, layer by layer, to produce an output (or prediction). Each layer applies certain computations, and its output is passed to the next layer until the final output is generated.

Types of Outputs:

Binary Classification: Produces a single probability between 0 and 1.
Multiclass Classification: Produces a distribution of probabilities summing to 1 (for different classes).
Regression: Generates continuous numerical predictions.


Example in PyTorch

Binary Classification Example:


# Create binary classification model
model = nn.Sequential(
    nn.Linear(6, 4),  # First linear layer
    nn.Linear(4, 1),  # Second linear layer
    nn.Sigmoid()      # Sigmoid activation for binary output
)

# Pass input data through the model
output = model(input_data)
print(output)




Multiclass Classification Example:

# Create multiclass classification model with 3 classes
n_classes = 3
model = nn.Sequential(
    nn.Linear(6, 4),    # First linear layer
    nn.Linear(4, n_classes),  # Second linear layer
    nn.Softmax(dim=-1)  # Softmax for multiclass output
)

# Forward pass
output = model(input_data)
print(output)

--------------------------------------------------------


Activation Functions
Limitations of Sigmoid and Softmax:
Sigmoid: Bounded between 0 and 1, can be used anywhere in the network.
Problem: Gradients approach zero for very low/high values of x, leading to saturation and vanishing gradients during backpropagation. This also affects Softmax.

Introducing ReLU (Rectified Linear Unit):
Function: f(x) = max(x, 0)
For positive inputs: output = input.
For negative inputs: output = 0.
Solves the vanishing gradient issue.
In PyTorch: nn.ReLU().

Introducing Leaky ReLU:
Similar to ReLU, but for negative inputs, it multiplies the input by a small coefficient (default 0.01).
Ensures gradients for negative inputs are never zero.
In PyTorch: nn.LeakyReLU(negative_slope=0.05).


Neural Network Architecture

Neurons and Linear Layers:
Linear layers: Fully connected, each neuron computes a linear operation.
A neuron has N+1 learnable parameters, where N is the number of outputs from the previous layer.

Hidden Layers:
The number of hidden layers can be adjusted, increasing model capacity but also the number of parameters.

Parameter Calculation:
Use .numel() in PyTorch to calculate the number of parameters.
Example: A model with nn.Linear(8, 4) and nn.Linear(4, 2) has 46 learnable parameters.


Learning Rate and Momentum
Learning rate (lr): Controls step size.
Momentum: Controls optimizer inertia (helps avoid getting stuck in local minima).
SGD optimizer in PyTorch: optim.SGD(model.parameters(), lr=0.01, momentum=0.95).
Typical lr values: 10^-2 to 10^-4. Momentum: 0.85 to 0.99.


Layer Initialization and Transfer Learning

Layer Initialization:
Weights are initialized to small values to prevent exploding outputs.
Example method: uniform distribution.

Transfer Learning and Fine-tuning:
Transfer learning: Reusing a trained model for a similar task.
Fine-tuning: Freezing early layers and training layers closer to the output with a smaller learning rate.

--------------------------------------------------------

Loading Data:

We used pandas to load a CSV file containing an animals dataset. Then, we defined our features (X) and target values (y).

animals = pd.read_csv('animals.csv')
features = animals.iloc[:, 1:-1].to_numpy()
target = animals.iloc[:, -1].to_numpy()


TensorDataset and DataLoader:
We converted our data into tensors using TensorDataset, which creates a dataset suitable for use in PyTorch models.
We then utilized DataLoader to handle batching and shuffling of the data during training.

from torch.utils.data import TensorDataset, DataLoader

dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

Batch Processing:
We iterated over the DataLoader to access our batched inputs and labels, preparing the data for training.

for batch_inputs, batch_labels in dataloader:
    print(batch_inputs, batch_labels)