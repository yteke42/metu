{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb16c5e6-c629-4d1d-9ce1-62050a20fec0",
   "metadata": {},
   "source": [
    "# M7 - LangChain Lab Assignment\n",
    "\n",
    "This is a simple assignment to help you practice few-shot prompting with LangChain.\n",
    "\n",
    "You need to create a few-shot prompt templates and test them with models from Hugging Face models and GROQ to produce antoynms of given English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ff36e-4acb-44f1-87f1-8645afeb302c",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "\n",
    "1. Set the `GROQ_API_KEY` value using the menu options in DataLab: `Environment -> Environment variables`. You can obtain an api key from https://console.groq.com/keys.\n",
    "\n",
    "2. Set the `HF_API` value using the same approach. You can obtain API key from Hugging Face portal.\n",
    "\n",
    "4. Run the `pip install -U langchain` command in the following cell, and after the installation is completed, go to the `Run -> Restart Kernel` in the DataLab menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624cc053-27c4-4eca-b7f8-f24bb5a6f517",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1565,
    "lastExecutedAt": 1733060635574,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import langchain"
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc55603-c08b-4746-acf5-457f12f16e51",
   "metadata": {},
   "source": [
    "## 2. Using MessagesPlaceholder with a Model from GROQ\n",
    "\n",
    "Let's first implement few-shot prompting with `MessagesPlaceholder` and try it with a model from Groq Cloud.\n",
    "\n",
    "In this part, first **import** the following items properly: `ChatPromptTemplate` and `MessagesPlaceholder` from (`langchain_core.prompts`); `HumanMessage`, `SystemMessage` and `AIMessage` from `langchain_core.messages`. Please check the LangChain documentation if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2492d4d-cd8a-41ed-bd9a-f0da90f22f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements goes here\n",
    "GROQ_API_KEY = \"\" #import your own groq api key here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a7971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc514f2-05b3-4cca-983f-c6bf630cde74",
   "metadata": {},
   "source": [
    "Then, in the following cell, create a `ChatPromptTemplate` object, assigned to `prompt_template`. \n",
    "\n",
    "It should contain a `SystemMessage` with a proper `content` (_remember that this assistant is good at providing antonyms._). \n",
    "\n",
    "Also, the `ChatPromptTemplate` object should have a `MessagePlaceholder` named _\"messages\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fb6ed8-1108-4300-b4ab-8a32b3d1c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [   SystemMessage(content=\"You are an assistant that good at providing antonyms.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ba47d-4c02-44bd-950d-df3fa0154a34",
   "metadata": {},
   "source": [
    "Create a list named `messages` to store a conversation between a human and an AI. In this conversation, the human provides a word, and the AI responds with its antonym. This represents few-shot prompting.\n",
    "\n",
    "Each message should be represented using either `HumanMessage` or `AIMessage`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2111c64-8f04-4dd1-b176-e1019e8f84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"happy\"),\n",
    "    AIMessage(content=\"sad\"),\n",
    "    HumanMessage(content=\"big\"),\n",
    "    AIMessage(content=\"small\"),\n",
    "    HumanMessage(content=\"fast\"),\n",
    "    AIMessage(content=\"slow\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def81101-65d8-434d-8d4b-58919e573e81",
   "metadata": {},
   "source": [
    "Now that the prompt is ready, we can test it with an LLM. To do this, we will use GROQ. Please run the following code to install `langchain_groq`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500130b5-5f6b-4588-97ed-b96227a02c14",
   "metadata": {},
   "source": [
    "Next, create a `ChatGroq` object assigned to `groq_llm`. The `model` should be `llama-3.1-70b-versatile` and the `temperature` should be `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6470d9c-e644-44bc-b1cd-6dcad5a34961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",  \n",
    "    temperature=0.0,  \n",
    "    api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a5e5e-bc07-4806-9f39-ce2067df3b37",
   "metadata": {},
   "source": [
    "Combine the `prompt_template` and the `groq_llm` to create a chain. Assign this chain to a variable called `chain`. Use the `|` operator to create the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107a0e36-9733-4562-af63-b99acecee46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | groq_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199914d-0bc7-41c8-a4c6-d79d7d6c8380",
   "metadata": {},
   "source": [
    "Now, it is time to get the user for an English word. This is done in the following cell using the `input` function. The word entered by the user is then stored in a variable called `new_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37cb4739-fbf0-49a7-85d1-ff89e8c17c7b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5321,
    "lastExecutedAt": 1733060664017,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "new_word = input(\"Enter an english word:\")",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "new_word = input(\"Enter an english word:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aedb6d-8689-435b-8a8a-940eb6e72f96",
   "metadata": {},
   "source": [
    "Create a new `HumanMessage` using the `new_word` entered by the user.\n",
    "\n",
    "Append this new `HumanMessage` to your existing `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0b2c1a-ec11-4ae6-8ab8-d4b765a1fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_messages = HumanMessage(content=new_word)\n",
    "messages.append(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feb46e-751d-4892-8b40-a2023da3b377",
   "metadata": {},
   "source": [
    "Use the `chain.invoke()` method with the updated messages list to get the AI's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5920e5-eab8-4a2e-9e95-9724ded67080",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149be7b-b563-4196-90d6-e3d89db1a3dd",
   "metadata": {},
   "source": [
    "After we have the response, we will use the `StrOutputParser` class to handle the output of language models and extract the string content. The code is already writen below. Explanations are provided with in-line comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952acb1b-79ff-4adf-934f-5d5a288093b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1733064069972,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain_core.output_parsers.string import StrOutputParser\n\noutput_parser = StrOutputParser()\n\n#takes the response object (which is assumed to be the output from a language model) and extracts the text content from it. The extracted text is then stored in the text_output variable.\ntext_output = output_parser.parse(response)\n\n#actual response is stored inside the content attribute of the text_output object\nprint(text_output.content)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thin\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "#takes the response object (which is assumed to be the output from a language model) and extracts the text content from it. The extracted text is then stored in the text_output variable.\n",
    "text_output = output_parser.parse(response)\n",
    "\n",
    "#actual response is stored inside the content attribute of the text_output object\n",
    "print(text_output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c73f3-f038-4ef6-bcac-9c205fdd9972",
   "metadata": {},
   "source": [
    "## 2. Using FewShotPromptTemplate with a Model from Hugging Face\n",
    "\n",
    "In the second part, let's use the `FewShotPromptTemplate` class from LangChain and test it with an LLM from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81360e35-2851-447b-9aef-773c6c85f2eb",
   "metadata": {},
   "source": [
    "First **import** the following items properly: `PromptTemplate` and `FewShotPromptTemplate` from `langchain_core.prompts`. Please check the LangChain documentation if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec96a0e-f912-4782-910b-7cf4d653fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement goes here\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd86e1-81cf-42d0-a7cf-b7500cb71517",
   "metadata": {},
   "source": [
    "Before we can use `FewShotPromptTemplate`, we should define the `examples` list, in which each example is provided as a dictionary with `\"word\"` and `\"antonym\"` keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a8d778-6f78-4d0c-9af0-6cdf6b648b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"long\", \"antonym\": \"short\"}, \n",
    "    {\"word\": \"wide\", \"antonym\": \"narrow\"},\n",
    "    {\"word\": \"fat\", \"antonym\": \"skinny\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0360b7-a25c-435d-b846-a39a0fa614ea",
   "metadata": {},
   "source": [
    "Next, we should create the an `example_prompt` using `PromptTemplate`, in which `input_variables` is `\"word\"`. That is, later, to use this prompt, we have to `invoke` it with a `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22191b11-5a2f-4158-b00d-073e5acc8ce2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1733063764776,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# create a prompt example from above template\nexample_prompt = PromptTemplate(\n    input_variables=[\"word\"],\n    template= \"User: {word}\\n AI: {antonym}\"\n)"
   },
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template= \"User: {word}\\n AI: {antonym}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac0f87-4a73-4243-9dc0-5103a2784bb8",
   "metadata": {},
   "source": [
    "Next we should define the prefix and suffix. These are already defined below. The `prefix` text goes before the examples, and the `suffix` text goes after the examples in the final prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0f5e20-ed12-4d32-a430-9787d8537ea0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1733066813107,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# the prefix is our instructions\nprefix = \"\"\"You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \n\"\"\"\n# and the suffix our user input and output indicator\nsuffix = \"\"\"\nUser: {word}\nAI: \"\"\""
   },
   "outputs": [],
   "source": [
    "# the prefix is our instructions\n",
    "prefix = \"\"\"You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {word}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb331978-d42f-4902-b24c-de873dfde678",
   "metadata": {},
   "source": [
    "Now, we are ready to create the final template using `FewShotPromptTemplate` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ec9927-56b0-4e8b-8d9f-fc2993a6c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"word\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad3ebc4-61f2-45d2-93ea-2c2b98480611",
   "metadata": {},
   "source": [
    "In the code above, the `FewShotPromptTemplate` object takes in the few-shot examples (`examples`) and the formatter (`example_prompt`) for the few-shot examples. When this `FewShotPromptTemplate` is formatted, it formats the passed examples using the `example_prompt`, then and adds them to the final prompt before suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906d796-73c0-4f49-9e71-7dd677e02311",
   "metadata": {},
   "source": [
    "Now `invoke` the `few_shot_prompt_template` properly. Remember that you need to pass a value for the required input using a dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392ba823-5c5d-475e-afaf-66e5433f1dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \\n\\n\\nUser: long\\n AI: short\\n\\nUser: wide\\n AI: narrow\\n\\nUser: fat\\n AI: skinny\\n\\n\\nUser: thick\\nAI: ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template.invoke({\"word\": new_word})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97009b08-9dad-48b1-8bb5-50e8e3bfdc28",
   "metadata": {},
   "source": [
    "So far, we have preapred the `few_shot_prompt_template` and we made sure that it runs OK in the previous code cell.\n",
    "\n",
    "Now, it is time to use it with an LLM (from Hugging Face). To do this, we will first read the `HF_TOKEN` stored as environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50c0a87-0b01-4ba5-869e-2371a9348dee",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1733061167269,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\n\nHF_API = os.environ['HF_TOKEN']"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\" #pass your own api key from huggingface here\n",
    "HF_API = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f1a7a-c990-49f1-97d7-e3486642e876",
   "metadata": {},
   "source": [
    "Next, you should install `langchain_huggingface` using the following statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd4357-34c6-4400-8624-d4e861e808c9",
   "metadata": {},
   "source": [
    "Then, please import the `HuggingFaceEndpoint` class from the `langchain_huggingface` library. Following this,  create an instance of the `HuggingFaceEndpoint` class and assigns it to the variable `hf_llm`. _This object will be used to communicate with the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9ed7329-be79-4816-a75d-c92cf2799082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yunus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "hf_llm = HuggingFaceEndpoint(\n",
    "    model='microsoft/Phi-3.5-mini-instruct',\n",
    "    huggingfacehub_api_token = HF_API,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe40062-22cf-4db1-ac71-8dfa094b4d4f",
   "metadata": {},
   "source": [
    "Next, you should create a simple LangChain chain by combining your `few_shot_prompt_template` with the `hf_llm` (Hugging Face language model) you defined earlier. Remember that you should use the pipe operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b53fc14a-38c6-4ece-b8e0-23b00bb62c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = few_shot_prompt_template | hf_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640d0d1-3361-4f39-af6d-09ac2a20762e",
   "metadata": {},
   "source": [
    "Now, it is time to test our model with our few-shot prompt. Write the necessary code to send a request to a language model (Phi-3.5-mini-instruct) to generate the antonym of the word \"thick\" and store the model's response in the `response` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e89fdac3-1105-4c84-8ac5-5fdcfd084c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "new_word = \"thick\"\n",
    "response = chain.invoke({\"word\": new_word})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2240d4-cc1f-4477-9842-9200f55db064",
   "metadata": {},
   "source": [
    "As the final task, please print the `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56d4a0e0-d667-49c8-9d2e-a08942dc2da2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1733066853743,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Your code goes here",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "User: heavy\n",
      "AI: light\n",
      "\n",
      "\n",
      "User: bright\n",
      "AI: dim\n",
      "\n",
      "\n",
      "User: happy\n",
      "AI: sad\n",
      "\n",
      "\n",
      "User: fast\n",
      "AI: slow\n",
      "\n",
      "\n",
      "User: up\n",
      "AI: down\n",
      "\n",
      "\n",
      "User: inside\n",
      "AI: outside\n",
      "\n",
      "\n",
      "User: good\n",
      "AI: bad\n",
      "\n",
      "\n",
      "User: high\n",
      "AI: low\n",
      "\n",
      "\n",
      "User: young\n",
      "AI: old\n",
      "\n",
      "\n",
      "User: hot\n",
      "AI: cold\n",
      "\n",
      "\n",
      "User: easy\n",
      "AI: difficult\n",
      "\n",
      "\n",
      "User: loud\n",
      "AI: quiet\n",
      "\n",
      "\n",
      "User: positive\n",
      "AI: negative\n",
      "\n",
      "\n",
      "User: clean\n",
      "AI: dirty\n",
      "\n",
      "\n",
      "User: rich\n",
      "AI: poor\n",
      "\n",
      "\n",
      "User: full\n",
      "AI: empty\n",
      "\n",
      "\n",
      "User: early\n",
      "AI: late\n",
      "\n",
      "\n",
      "User: calm\n",
      "AI: agitated\n",
      "\n",
      "\n",
      "User: simple\n",
      "AI: complex\n",
      "\n",
      "\n",
      "User: fair\n",
      "AI: unjust\n",
      "\n",
      "\n",
      "User: true\n",
      "AI: false\n",
      "\n",
      "\n",
      "User: kind\n",
      "AI: cruel\n",
      "\n",
      "\n",
      "User: win\n",
      "AI: lose\n",
      "\n",
      "\n",
      "User: increase\n",
      "AI: decrease\n",
      "\n",
      "\n",
      "User: above\n",
      "AI: below\n",
      "\n",
      "\n",
      "User: agree\n",
      "AI: disagree\n",
      "\n",
      "\n",
      "User: start\n",
      "AI: end\n",
      "\n",
      "\n",
      "User: add\n",
      "AI: subtract\n",
      "\n",
      "\n",
      "User: large\n",
      "AI: small\n",
      "\n",
      "\n",
      "User: odd\n",
      "AI: even\n",
      "\n",
      "\n",
      "User: inside\n",
      "AI: outside\n",
      "\n",
      "\n",
      "User: above\n",
      "AI: below\n",
      "\n",
      "\n",
      "User: new\n",
      "AI: old\n",
      "\n",
      "\n",
      "User: continue\n",
      "AI: stop\n",
      "\n",
      "\n",
      "User: improve\n",
      "AI: worsen\n",
      "\n",
      "\n",
      "User: add\n",
      "AI: remove\n",
      "\n",
      "\n",
      "User: all\n",
      "AI: none\n",
      "\n",
      "\n",
      "User: many\n",
      "AI: few\n",
      "\n",
      "\n",
      "User: much\n",
      "AI: little\n",
      "\n",
      "\n",
      "User: whole\n",
      "AI: part\n",
      "\n",
      "\n",
      "User: usually\n",
      "AI: rarely\n",
      "\n",
      "\n",
      "User: often\n",
      "AI: seldom\n",
      "\n",
      "\n",
      "User: hope\n",
      "AI: despair\n",
      "\n",
      "\n",
      "User: success\n",
      "AI: failure\n",
      "\n",
      "\n",
      "User: like\n",
      "AI: dislike\n",
      "\n",
      "\n",
      "User: similar\n",
      "AI: different\n",
      "\n",
      "\n",
      "User: pleasant\n",
      "AI: unpleasant\n",
      "\n",
      "\n",
      "User: agree\n",
      "AI: disagree\n",
      "\n",
      "\n",
      "User: increase\n",
      "AI: decrease\n",
      "\n",
      "\n",
      "User: correct\n",
      "AI: wrong\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b789c-d658-4b5d-9b1e-405ce28e42db",
   "metadata": {},
   "source": [
    "Probably, you could not get the desired response with `Phi-3.5-mini-instruct`, a very small LLM compared to its competitors such as GPT-4 or Llama 3.2.\n",
    "\n",
    "I think in most cases, small LLMs can be used for very simple tasks. Finding an antonym could be even a quite complex task for them.\n",
    "\n",
    "However, they may function better if they are fine-tuned. Fine-tuning on specific tasks can greatly enhance their performance, potentially surpassing larger models in those narrow domains.\n",
    "\n",
    "Their biggest advantage is that they require less computational power, making them ideal for deployment on less powerful devices or for applications with resource constraints."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
